{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16cced88-6145-4c0a-8cbe-87a2339bea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=51>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o501.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error\n",
      "Stoped the streaming query and the spark context\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col,  explode, split, concat, col, lit\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Streaming_pipeline\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Read the whole dataset as a batch\n",
    "df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka1:9093\").option(\"failOnDataLoss\", \"false\") \\\n",
    "        .option(\"subscribe\", \"games_details\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "lines = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "words = lines.select(\n",
    "        explode(\n",
    "            split(lines.value, \" \")\n",
    "        ).alias(\"word\")\n",
    "    )\n",
    "\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "query = wordCounts \\\n",
    "              .select(concat(col(\"word\"), lit(\" \"), col(\"count\")).alias(\"value\")) \\\n",
    "              .writeStream \\\n",
    "              .format(\"kafka\") \\\n",
    "              .option(\"kafka.bootstrap.servers\", \"kafka1:9093\").option(\"failOnDataLoss\", \"false\") \\\n",
    "              .option(\"checkpointLocation\", \"/home/jovyan/checkpoint/games_details\")\\\n",
    "              .option(\"topic\", \"games_details\") \\\n",
    "              .outputMode(\"complete\") \\\n",
    "              .start()\n",
    "\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")\n",
    "except:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Unexpected error\")\n",
    "    print(\"Stoped the streaming query and the spark context\")\n",
    "\n",
    "\n",
    "# Define the schema for games_details.json\n",
    "# gamesDetailsSchema = StructType([\n",
    "#     StructField(\"game_id\", StringType(), True),\n",
    "#     StructField(\"player_id\", StringType(), True),\n",
    "#     StructField(\"team_id\", StringType(), True),\n",
    "#     StructField(\"player_name\", StringType(), True),\n",
    "#     StructField(\"team_abbreviation\", StringType(), True),\n",
    "#     StructField(\"min\", StringType(), True),\n",
    "#     StructField(\"ast\", IntegerType(), True),\n",
    "#     StructField(\"stl\", IntegerType(), True),\n",
    "#     StructField(\"pf\", IntegerType(), True)\n",
    "# ])\n",
    "\n",
    "# # Read from a source \n",
    "# sdf = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1) \\\n",
    "#         .json(\"/home/jovyan/data/games_details\")\n",
    "\n",
    "# # Write to a sink - here, the output is memory (only for testing). The query name (i.e., activity_counts) will be the Spark SQL table name.\n",
    "# gamesDetailsQuery = activityCounts.writeStream.queryName(\"games_details_query\") \\\n",
    "#                     .format(\"memory\").outputMode(\"complete\") \\\n",
    "#                     .start()\n",
    "# # Testing \n",
    "# for x in range(10):\n",
    "#     spark.sql(\"SELECT * FROM games_details_query\").show()\n",
    "#     sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6348e31-31c1-4cef-a47d-c2daebbd9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c2412-6e3a-46e6-a09a-7ad73e1d154b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
