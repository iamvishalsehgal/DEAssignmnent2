{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16cced88-6145-4c0a-8cbe-87a2339bea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- raw_json: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- game_id: string (nullable = true)\n",
      " |    |    |-- player_id: string (nullable = true)\n",
      " |    |    |-- team_id: string (nullable = true)\n",
      " |    |    |-- player_name: string (nullable = true)\n",
      " |    |    |-- team_abbreviation: string (nullable = true)\n",
      " |    |    |-- min: string (nullable = true)\n",
      " |    |    |-- ast: integer (nullable = true)\n",
      " |    |    |-- stl: integer (nullable = true)\n",
      " |    |    |-- pf: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col,  explode, split, concat, col, lit, from_json\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, IntegerType, ArrayType, DoubleType\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "### first we define the schema for our dataaa\n",
    "# Define the schema\n",
    "dataSchemaString = StructType([\n",
    "    StructField(\"game_id\", StringType(), True),\n",
    "    StructField(\"player_id\", StringType(), True),\n",
    "    StructField(\"team_id\", StringType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"team_abbreviation\", StringType(), True),\n",
    "    StructField(\"min\", StringType(), True),\n",
    "    StructField(\"ast\", IntegerType(), True),\n",
    "    StructField(\"stl\", IntegerType(), True),\n",
    "    StructField(\"pf\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "schema = StructType(\n",
    "        [\n",
    "             StructField(\"raw_json\", ArrayType(dataSchemaString), True),     \n",
    "        ]\n",
    ")\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"JsonSchemaCreator\").getOrCreate()\n",
    "\n",
    "# Use the schema to create an empty DataFrame\n",
    "empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# Print the schema\n",
    "empty_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e74e3a5a-b6b8-40ee-9d41-28c46d08a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a function which add the NBA results\n",
    "def write_nba_results(df, batch_id):\n",
    "    print('posted')\n",
    "    df.show()\n",
    "    df \\\n",
    "      .write.format('bigquery') \\\n",
    "      .option('table', 'dataengineeringcourse2023.Output_processing_pipeline.new_nba_results') \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6348e31-31c1-4cef-a47d-c2daebbd9c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "After starting first spark session\n",
      "posted\n",
      "+-------+---------+-------+-----------+-----------------+---+---+---+---+\n",
      "|game_id|player_id|team_id|player_name|team_abbreviation|min|ast|stl| pf|\n",
      "+-------+---------+-------+-----------+-----------------+---+---+---+---+\n",
      "+-------+---------+-------+-----------+-----------------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Streaming_pipeline\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"temp_de2023_2124849\"  \n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "## so here is the first step of the stream preprocessing pipeline, we read the data which is produced by producer (in our case our laptop )\n",
    "# Read the whole dataset as a batch\n",
    "df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "        .option(\"failOnDataLoss\", \"false\") \\\n",
    "        .option(\"subscribe\", \"games_details\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "print(\"After starting first spark session\")\n",
    "\n",
    "df = df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\")) \\\n",
    "          .withColumn(\"exploded\", explode(col(\"parsed_value.raw_json\"))) \\\n",
    "          .drop(col(\"parsed_value\")).select(\"exploded.*\")\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(write_nba_results) \\\n",
    "    .start()\n",
    "## some preprocessing of data \n",
    "# Split the concatenated data into individual JSON strings\n",
    "# json_strings = concatenated_data.split('\\n')\n",
    "\n",
    "## here is the checkpoint \n",
    "## A streaming query saves its progress information into the checkpoint\n",
    "## location. Upon failure, this metadata is used to restart the failed\n",
    "## query exactly where it left off.\n",
    "## So in our case, we keep adding the data every day and if something break, we will use the metadata from the query to resume where we left\n",
    "\n",
    "\n",
    "try:\n",
    "    query.awaitTermination\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")\n",
    "except:\n",
    "    query.stop()\n",
    "    \n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Unexpected error\")\n",
    "    print(\"Stoped the streaming query and the spark context\")\n",
    "\n",
    "# print('Before the gamesDetailsSchema')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a901d182-8282-404c-abcf-0aac760ef45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
