{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16cced88-6145-4c0a-8cbe-87a2339bea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col,  explode, split, concat, col, lit, from_json, max, avg\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, IntegerType, ArrayType, DoubleType\n",
    "from time import sleep\n",
    "\n",
    "### First we define the schema for our data\n",
    "dataSchemaString = StructType([\n",
    "    StructField(\"game_id\", StringType(), True),\n",
    "    StructField(\"player_id\", StringType(), True),\n",
    "    StructField(\"team_id\", StringType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"team_abbreviation\", StringType(), True),\n",
    "    StructField(\"min\", StringType(), True),\n",
    "    StructField(\"ast\", IntegerType(), True),\n",
    "    StructField(\"stl\", IntegerType(), True),\n",
    "    StructField(\"pf\", IntegerType(), True),\n",
    "    StructField(\"eventTime\", LongType(), True)\n",
    "])\n",
    "\n",
    "### create a function that add the NBA results\n",
    "def write_nba_results(df, batch_id):\n",
    "    print('posted')\n",
    "    print(batch_id)\n",
    "    df.show()\n",
    "    df \\\n",
    "      .write.format('bigquery') \\\n",
    "      .option('table', 'dataengineeringcourse2023.Output_processing_pipeline.new_nba_results') \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6348e31-31c1-4cef-a47d-c2daebbd9c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df1 schema\n",
      "root\n",
      " |-- from_json(value): struct (nullable = true)\n",
      " |    |-- game_id: string (nullable = true)\n",
      " |    |-- player_id: string (nullable = true)\n",
      " |    |-- team_id: string (nullable = true)\n",
      " |    |-- player_name: string (nullable = true)\n",
      " |    |-- team_abbreviation: string (nullable = true)\n",
      " |    |-- min: string (nullable = true)\n",
      " |    |-- ast: integer (nullable = true)\n",
      " |    |-- stl: integer (nullable = true)\n",
      " |    |-- pf: integer (nullable = true)\n",
      " |    |-- eventTime: long (nullable = true)\n",
      "\n",
      "Sdf schema\n",
      "root\n",
      " |-- game_id: string (nullable = true)\n",
      " |-- player_id: string (nullable = true)\n",
      " |-- team_id: string (nullable = true)\n",
      " |-- player_name: string (nullable = true)\n",
      " |-- team_abbreviation: string (nullable = true)\n",
      " |-- min: string (nullable = true)\n",
      " |-- ast: integer (nullable = true)\n",
      " |-- stl: integer (nullable = true)\n",
      " |-- pf: integer (nullable = true)\n",
      " |-- eventTime: long (nullable = true)\n",
      "\n",
      "posted\n",
      "0\n",
      "posted\n",
      "2\n",
      "posted\n",
      "5\n",
      "+---------+-----------+----------+\n",
      "|player_id|player_name|max_points|\n",
      "+---------+-----------+----------+\n",
      "+---------+-----------+----------+\n",
      "\n",
      "+---------+-----------+----------+\n",
      "|player_id|player_name|max_points|\n",
      "+---------+-----------+----------+\n",
      "+---------+-----------+----------+\n",
      "\n",
      "+---------+-----------+----------+\n",
      "|player_id|player_name|max_points|\n",
      "+---------+-----------+----------+\n",
      "+---------+-----------+----------+\n",
      "\n",
      "posted\n",
      "1\n",
      "posted\n",
      "6\n",
      "posted\n",
      "3\n",
      "+---------+-----------+----------+\n",
      "|player_id|player_name|max_points|\n",
      "+---------+-----------+----------+\n",
      "+---------+-----------+----------+\n",
      "\n",
      "+---------+-----------+----------+\n",
      "|player_id|player_name|max_points|\n",
      "+---------+-----------+----------+\n",
      "+---------+-----------+----------+\n",
      "\n",
      "+---------+------------------+----------+\n",
      "|player_id|       player_name|max_points|\n",
      "+---------+------------------+----------+\n",
      "|      123|Isabella Hernandez|         4|\n",
      "|      114|       James Moore|         4|\n",
      "|      119|      Mia Thompson|         4|\n",
      "|      116|    Liam Rodriguez|         3|\n",
      "|      111|        Ella Davis|         3|\n",
      "|      124|      Daniel Davis|         3|\n",
      "|      120|       Noah Wilson|         3|\n",
      "|      117|Charlotte Martinez|         2|\n",
      "|      112|    William Taylor|         2|\n",
      "|      115|      Chloe Garcia|         2|\n",
      "|      125|    Madison Taylor|         2|\n",
      "|      121|      Sophia Adams|         2|\n",
      "|      113|        Ava Wilson|         1|\n",
      "|      122|  Alexander Garcia|         1|\n",
      "|      118|   Lucas Hernandez|         1|\n",
      "+---------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Streaming_pipeline\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"temp_de2023_2124849\"  \n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "## so here is the first step of the stream preprocessing pipeline, we read the data which is produced by producer (in our case our laptop )\n",
    "# Read the whole dataset as a batch\n",
    "kafkaStream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"subscribe\", \"games_details\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "df = kafkaStream.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df1 = df.select(from_json(df.value, dataSchemaString.simpleString()))\n",
    "\n",
    "sdf = df1.select(col(\"from_json(value).*\"))\n",
    "\n",
    "print(\"Sdf schema\")\n",
    "\n",
    "sdf.printSchema()\n",
    "\n",
    "# Group by player_id and find the maximum points scored\n",
    "result = sdf.groupBy(\"player_id\", \"player_name\").agg(max(\"pf\").alias(\"max_points\"))\n",
    "\n",
    "# Order the result in descending order based on max_points\n",
    "result_ordered = result.orderBy(col(\"max_points\").desc())\n",
    "\n",
    "query = result_ordered \\\n",
    "    .writeStream.outputMode(\"complete\") \\\n",
    "    .foreachBatch(write_nba_results) \\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    query.awaitTermination\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")\n",
    "except:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Unexpected error\")\n",
    "    print(\"Stoped the streaming query and the spark context\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a901d182-8282-404c-abcf-0aac760ef45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef7ca2f-a50b-49f1-b9be-d4094083d963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
